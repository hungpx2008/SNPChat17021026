"""
Chat tasks ‚Äî Queue: chat_priority

Tasks:
  - process_chat_response: Chunk ‚Üí Embed ‚Üí Store in Qdrant
  - store_memory: Save long-term memory to Mem0
  - rag_document_search: RAG search across uploaded documents
  - process_feedback: Self-correction via user feedback
"""
import logging
import os
import re
from concurrent.futures import ThreadPoolExecutor
from typing import Any
from uuid import uuid4

from .celery_app import celery_app

logger = logging.getLogger(__name__)

BACKEND_INTERNAL_URL = os.getenv("BACKEND_INTERNAL_URL", "http://backend:8000")
MEM0_DEFAULT_URL = "http://mem0:8000"


# =============================================================================
# üî¥ QUEUE: chat_priority ‚Äî Chat real-time
# =============================================================================

@celery_app.task(name="src.worker.tasks.process_chat_response", bind=True, max_retries=3)
def process_chat_response(
    self,
    session_id: str,
    message_id: str,
    content: str,
    role: str,
    user_id: str | None = None,
    department: str | None = None,
) -> dict[str, Any]:
    """
    X·ª≠ l√Ω tin nh·∫Øn chat: c·∫Øt ƒëo·∫°n ‚Üí embedding ‚Üí l∆∞u v√†o Qdrant.
    """
    logger.info(f"[chat_priority] Processing message {message_id} for session {session_id}")
    try:
        from src.core.qdrant_setup import get_qdrant_client
        from qdrant_client.http import models as qmodels
        from .helpers import _smart_chunk

        # 1. Chunk text
        chunks = _smart_chunk(content, chunk_size=512, overlap=50)
        if not chunks:
            return {"status": "ok", "message_id": message_id, "chunks": 0}

        # 2. Embed each chunk via Mem0 ‚Äî parallel with ThreadPoolExecutor
        mem0_url = os.getenv("MEM0_URL", MEM0_DEFAULT_URL)
        embed_url = f"{mem0_url.rstrip('/')}/embed"

        def _embed_chunk(chunk_text: str) -> list[float] | None:
            import httpx
            resp = httpx.post(embed_url, json={"text": chunk_text}, timeout=30.0)
            if resp.status_code != 200:
                logger.warning(f"[mem0] Embed failed: {resp.status_code}")
                return None
            return resp.json()["vector"]

        chunk_texts = [t for t, _ in chunks]
        with ThreadPoolExecutor(max_workers=min(len(chunk_texts), 8)) as pool:
            vectors = list(pool.map(_embed_chunk, chunk_texts))

        if any(v is None for v in vectors):
            return {"status": "warning", "message_id": message_id}

        # 3. Store vectors in Qdrant
        qdrant = get_qdrant_client()
        points = []
        for i, ((chunk_text, page_num), vector) in enumerate(zip(chunks, vectors)):
            point_id = str(uuid4())
            points.append(qmodels.PointStruct(
                id=point_id,
                vector=vector,
                payload={
                    "content": chunk_text,
                    "session_id": session_id,
                    "message_id": message_id,
                    "user_id": user_id or "",
                    "role": role,
                    "department": department or "",
                    "chunk_index": i,
                },
            ))

        if points:
            qdrant.upsert(collection_name="chat_chunks", points=points)
            logger.info(f"[chat_priority] Stored {len(points)} chunks for message {message_id}")

        return {"status": "ok", "message_id": message_id, "chunks": len(points)}
    except Exception as exc:
        logger.exception(f"Error processing chat response: {exc}")
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)


@celery_app.task(name="src.worker.tasks.store_memory", bind=True, max_retries=3)
def store_memory(
    self,
    user_id: str,
    content: str,
    role: str,
    session_id: str,
    department: str | None = None,
) -> dict[str, Any]:
    """
    L∆∞u k√Ω ·ª©c d√†i h·∫°n v√†o Mem0.
    POST to Mem0 /memories API with correct MemoryCreate schema.
    """
    logger.info(f"[chat_priority] Storing memory for user {user_id}")
    try:
        import httpx
        mem0_url = os.getenv("MEM0_URL", MEM0_DEFAULT_URL)

        with httpx.Client(timeout=30.0) as client:
            resp = client.post(
                f"{mem0_url.rstrip('/')}/memories",
                json={
                    "messages": [{"role": role, "content": content}],
                    "user_id": user_id,
                    "metadata": {
                        "session_id": session_id,
                        "department": department,
                    },
                },
            )
            if resp.status_code in (200, 201):
                logger.info(f"[mem0] Memory stored for user {user_id}")
                return {"status": "ok", "user_id": user_id}
            else:
                logger.warning(f"[mem0] Store failed: {resp.status_code} {resp.text[:200]}")
                return {"status": "warning", "user_id": user_id, "detail": resp.text[:200]}

    except Exception as exc:
        logger.exception(f"Error storing memory: {exc}")
        raise self.retry(exc=exc, countdown=2 ** self.request.retries)


# ---------------------------------------------------------------------------
# RAG helper functions (extracted to reduce cognitive complexity)
# ---------------------------------------------------------------------------

def _build_qdrant_filter(user_id: str | None, department: str | None):
    """Build Qdrant security filter for RAG search."""
    from qdrant_client.models import Filter, FieldCondition, MatchValue

    if not user_id and not department:
        return None
    should_conditions = []
    if user_id:
        should_conditions.append(
            FieldCondition(key="user_id", match=MatchValue(value=user_id))
        )
    if department:
        should_conditions.append(Filter(must=[
            FieldCondition(key="department", match=MatchValue(value=department)),
            FieldCondition(key="is_public", match=MatchValue(value=True)),
        ]))
    return Filter(should=should_conditions) if should_conditions else None


def _extract_node_metadata(node_with_score) -> tuple[str, Any]:
    """Extract filename and page display from a LlamaIndex node."""
    try:
        meta = node_with_score.node.metadata or {}
    except Exception:
        meta = {}
    fname = meta.get("source_file", "T√†i li·ªáu")
    page = (
        meta.get("page_number") or meta.get("page")
        or meta.get("page_no") or meta.get("pageIndex")
        or meta.get("page_index")
    )
    try:
        page = int(page) if page is not None else None
    except Exception:
        page = None
    page_display = page if page is not None else meta.get("chunk_index", "?")
    return fname, page_display


def _extract_snippet(node_with_score) -> str:
    """Extract clean text content from a LlamaIndex node (no metadata)."""
    if hasattr(node_with_score, "text") and node_with_score.text:
        return node_with_score.text.strip()
    try:
        return node_with_score.node.get_content(metadata_mode="none").strip()
    except Exception:
        return str(node_with_score).strip()


def _build_context_and_citations(top_nodes: list) -> tuple[list[dict[str, Any]], list[str]]:
    """Parse retrieval nodes into deduplicated citations and context blocks."""
    citations: list[dict[str, Any]] = []
    context_blocks: list[str] = []
    seen_citations: set[str] = set()
    seen_content: set[str] = set()  # ‚Üê Th√™m dedup theo n·ªôi dung

    for idx, node in enumerate(top_nodes, start=1):
        fname, page_display = _extract_node_metadata(node)
        cite_key = f"{fname}|{page_display}"
        snippet = _extract_snippet(node)

        # T·∫°o content hash ƒë·ªÉ check tr√πng l·∫∑p n·ªôi dung
        content_hash = hash(snippet.strip()[:200]) if snippet else None

        # Skip n·∫øu n·ªôi dung ƒë√£ xu·∫•t hi·ªán (dedup m·∫°nh h∆°n)
        if content_hash and content_hash in seen_content:
            continue

        if cite_key not in seen_citations:
            seen_citations.add(cite_key)
            score = getattr(node, "score", None)
            citations.append({
                "index": len(citations) + 1,
                "file": fname, "page": page_display,
                "score": round(score, 3) if score else None,
            })

        if snippet and content_hash:
            seen_content.add(content_hash)  # ‚Üê Mark n·ªôi dung ƒë√£ th·∫•y
            cite_idx = next(
                (c["index"] for c in citations if c["file"] == fname and c["page"] == page_display),
                len(citations),  # Fallback to last citation index
            )
            context_blocks.append(f"[{cite_idx}] {snippet}")
    return citations, context_blocks


def _gather_unified_context(question: str, session_id: str, user_id: str | None) -> dict[str, str]:
    """Collect long-term memory, session summary, and recent chat for unified prompting."""
    long_term_block = ""
    summary_block = ""
    recent_block = ""

    # Long-term memories via Mem0 (best effort)
    if user_id:
        try:
            import httpx
            mem0_url = os.getenv("MEM0_URL", MEM0_DEFAULT_URL)
            with httpx.Client(timeout=10.0) as client:
                resp = client.post(
                    f"{mem0_url.rstrip('/')}/search",
                    json={"query": question, "user_id": user_id, "limit": 5},
                )
                if resp.status_code == 200:
                    results = resp.json().get("results") or []
                    if results:
                        long_term_block = "\n".join(
                            f"- {item.get('text') or item.get('memory') or ''}" for item in results
                        ).strip()
        except Exception as e:
            logger.warning(f"[RAG] Mem0 long-term fetch failed: {e}")

    # Session summary + recent messages via DB (sync)
    try:
        from sqlalchemy import create_engine, text as sql_text

        db_url = os.getenv("DATABASE_URL", "").replace("postgresql+asyncpg://", "postgresql://")
        engine = create_engine(db_url)
        with engine.connect() as conn:
            # Summary
            res = conn.execute(
                sql_text("SELECT metadata FROM chat_sessions WHERE id = :sid"),
                {"sid": session_id},
            ).fetchone()
            if res and res[0]:
                meta = res[0]
                if isinstance(meta, dict) and meta.get("summary"):
                    summary_block = str(meta["summary"])

            # Recent messages (last 6)
            rows = conn.execute(
                sql_text(
                    "SELECT role, content FROM chat_messages "
                    "WHERE session_id = :sid ORDER BY created_at DESC LIMIT 6"
                ),
                {"sid": session_id},
            ).fetchall()
            if rows:
                rows = list(reversed(rows))
                recent_block = "\n".join(f"{r[0].upper()}: {r[1]}" for r in rows)
    except Exception as e:
        logger.warning(f"[RAG] Recent history fetch failed: {e}")

    return {
        "long_term_block": long_term_block,
        "summary_block": summary_block,
        "recent_block": recent_block,
    }


_RAG_SYSTEM_PROMPT = (
    "ü§ñ B·∫°n l√† AI Assistant th√¥ng minh v√† chuy√™n nghi·ªáp c·ªßa ChatSNP. "
    "Nhi·ªám v·ª•: bi·∫øn th√¥ng tin th√¥ th√†nh c√¢u tr·∫£ l·ªùi SINH ƒê·ªòNG, D·ªÑ HI·ªÇU, C√ì GI√Å TR·ªä.\n\n"

    "üìã QUY T·∫ÆC V√ÄNG:\n"
    "‚úÖ CH·ªà d√πng th√¥ng tin t·ª´ t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p\n"
    "‚úÖ Vi·∫øt b·∫±ng ti·∫øng Vi·ªát t·ª± nhi√™n, d·ªÖ hi·ªÉu\n"
    "‚úÖ T·∫°o c·∫•u tr√∫c r√µ r√†ng v·ªõi ti√™u ƒë·ªÅ ƒë·∫πp\n"
    "‚úÖ T√≥m t·∫Øt ng·∫Øn g·ªçn ·ªü ƒë·∫ßu, chi ti·∫øt b√™n d∆∞·ªõi\n"
    "‚ùå TUY·ªÜT ƒê·ªêI kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin\n"
    "‚ùå Kh√¥ng hi·ªÉn th·ªã metadata k·ªπ thu·∫≠t\n"
    "‚ùå Kh√¥ng copy nguy√™n vƒÉn c√¢u h·ªèi\n\n"

    "üé® C·∫§U TR√öC TR·∫¢ L·ªúI:\n"
    "**B∆∞·ªõc 1:** M·ªü ƒë·∫ßu b·∫±ng 1-2 c√¢u t√≥m t·∫Øt c·ªët l√µi üí°\n"
    "**B∆∞·ªõc 2:** Ph√¢n chia th√†nh c√°c m·ª•c ch√≠nh v·ªõi emoji v√† ti√™u ƒë·ªÅ ƒë·∫πp:\n"
    "   ‚Ä¢ üìä **S·ªë li·ªáu quan tr·ªçng** (n·∫øu c√≥)\n"
    "   ‚Ä¢ üîç **Chi ti·∫øt c·ª• th·ªÉ**\n"
    "   ‚Ä¢ ‚ö° **ƒêi·ªÉm n·ªïi b·∫≠t**\n"
    "**B∆∞·ªõc 3:** K·∫øt th√∫c b·∫±ng takeaway quan tr·ªçng üéØ\n\n"

    "üìù NGUY√äN T·∫ÆC VI·∫æT:\n"
    "‚Ä¢ D√πng **heading in ƒë·∫≠m** cho m·ª•c ch√≠nh\n"
    "‚Ä¢ Bullet points (‚Ä¢) cho danh s√°ch\n"
    "‚Ä¢ B·∫£ng markdown cho s·ªë li·ªáu ph·ª©c t·∫°p\n"
    "‚Ä¢ Emoji ph√π h·ª£p ƒë·ªÉ t·∫°o ƒëi·ªÉm nh·∫•n\n"
    "‚Ä¢ Tr√≠ch d·∫´n [1], [2]... sau m·ªói th√¥ng tin quan tr·ªçng\n\n"

    "üí´ V√ç D·ª§ M·∫™U:\n"
    "üí∞ **Bi·ªÉu gi√° d·ªãch v·ª• c·∫ßu b·∫øn** hi·ªán t·∫°i c√≥ nh·ªØng thay ƒë·ªïi quan tr·ªçng t·ª´ ƒë·∫ßu nƒÉm 2026.\n\n"
    "üìã **Chi ti·∫øt gi√° c∆∞·ªõc:**\n"
    "| Lo·∫°i h√†ng h√≥a | ƒê∆°n gi√° | Ghi ch√∫ |\n"
    "|---|---|---|\n"
    "| Container 20ft | 150.000 VNƒê | √Åp d·ª•ng t·ª´ 01/2026 [1] |\n"
    "| Container 40ft | 280.000 VNƒê | Gi√° ∆∞u ƒë√£i [1] |\n\n"
    "‚ö° **Nh·ªØng ƒëi·ªÉm c·∫ßn l∆∞u √Ω:**\n"
    "‚Ä¢ üÜì Mi·ªÖn ph√≠ l∆∞u b√£i trong 3 ng√†y ƒë·∫ßu ti√™n [2]\n"
    "‚Ä¢ üí∏ Ph·ª• ph√≠ THC √°p d·ª•ng cho h√†ng h√≥a v∆∞·ª£t t·∫£i [2]\n\n"
    "üéØ **K·∫øt lu·∫≠n:** Gi√° c∆∞·ªõc m·ªõi gi√∫p t·ªëi ∆∞u chi ph√≠ v·∫≠n chuy·ªÉn cho doanh nghi·ªáp."
)


def _synthesize_with_llm(
    question: str,
    context_text: str,
    *,
    long_term_block: str = "",
    summary_block: str = "",
    recent_block: str = "",
) -> str:
    """Call LLM (GPT-5 Nano via OpenRouter) to synthesize a clean answer."""
    import httpx

    openai_key = os.getenv("OPENAI_API_KEY", "")
    openai_base = os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1")
    llm_model = os.getenv("LLM_MODEL", "openai/gpt-5-nano")

    unified_context_parts = []
    if long_term_block:
        unified_context_parts.append("### Long-term Memory\n" + long_term_block)
    if summary_block:
        unified_context_parts.append("### T√≥m t·∫Øt h·ªôi tho·∫°i\n" + summary_block)
    if recent_block:
        unified_context_parts.append("### H·ªôi tho·∫°i g·∫ßn ƒë√¢y\n" + recent_block)
    unified_context_parts.append("### ƒêo·∫°n tr√≠ch t√†i li·ªáu (ƒë√£ ƒë√°nh s·ªë)\n" + context_text)
    unified_context = "\n\n".join(unified_context_parts)

    user_prompt = (
        f"üîç **C√¢u h·ªèi t·ª´ ng∆∞·ªùi d√πng:** {question}\n\n"
        "üìö **CONTEXT (∆∞u ti√™n theo th·ª© t·ª±):**\n\n"
        f"{unified_context}\n\n"
        "üéØ **Y√™u c·∫ßu:** H√£y bi·∫øn nh·ªØng th√¥ng tin th√¥ n√†y th√†nh c√¢u tr·∫£ l·ªùi CHUY√äN NGHI·ªÜP, "
        "SINH ƒê·ªòNG v√† D·ªÑ HI·ªÇU. T·∫≠p trung v√†o gi√° tr·ªã th·ª±c t·∫ø m√† ng∆∞·ªùi d√πng c·∫ßn bi·∫øt!"
    )
    with httpx.Client(timeout=60.0) as http_client:
        resp = http_client.post(
            f"{openai_base.rstrip('/')}/chat/completions",
            headers={"Authorization": f"Bearer {openai_key}", "Content-Type": "application/json"},
            json={
                "model": llm_model,
                "messages": [
                    {"role": "system", "content": _RAG_SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                "temperature": 0.3,
                "max_tokens": 1500,
            },
        )
        resp.raise_for_status()
        return resp.json()["choices"][0]["message"]["content"].strip()


def _build_fallback_answer(context_blocks: list[str]) -> str:
    """Build a clean fallback when LLM synthesis fails."""
    if not context_blocks:
        return (
            "üîç **K·∫øt qu·∫£ t√¨m ki·∫øm:**\n\n"
            "üòî Em ƒë√£ t√¨m ki·∫øm k·ªπ l∆∞·ª°ng trong to√†n b·ªô t√†i li·ªáu nh∆∞ng ch∆∞a t√¨m th·∫•y "
            "th√¥ng tin ph√π h·ª£p v·ªõi c√¢u h·ªèi c·ªßa b·∫°n.\n\n"
            "üí° **G·ª£i √Ω:**\n"
            "‚Ä¢ Th·ª≠ ƒë·∫∑t c√¢u h·ªèi chi ti·∫øt h∆°n\n"
            "‚Ä¢ Ki·ªÉm tra l·∫°i t·ª´ kh√≥a t√¨m ki·∫øm\n"
            "‚Ä¢ ƒê·∫£m b·∫£o t√†i li·ªáu li√™n quan ƒë√£ ƒë∆∞·ª£c upload\n\n"
            "ü§ù Em s·∫µn s√†ng h·ªó tr·ª£ b·∫°n t√¨m ki·∫øm th√¥ng tin kh√°c!"
        )

    lines = ["üìã **Th√¥ng tin li√™n quan t·ª´ t√†i li·ªáu:**\n"]

    for i, block in enumerate(context_blocks, 1):
        clean = re.sub(r'^\[\d+\]\s*', '', block).strip()
        lines.append(f"**{i}.** {clean}\n")

    lines.append("\nüí¨ **L∆∞u √Ω:** ƒê√¢y l√† th√¥ng tin th√¥ t·ª´ t√†i li·ªáu, b·∫°n c√≥ th·ªÉ h·ªèi c·ª• th·ªÉ h∆°n ƒë·ªÉ em t·ªïng h·ª£p chi ti·∫øt!")
    return "\n".join(lines)


def _format_citations_footer(citations: list[dict[str, Any]]) -> str:
    """Format citations into a beautiful markdown footer."""
    if not citations:
        return ""

    cite_lines = ["---", "üìö **Ngu·ªìn tham kh·∫£o:**"]
    for c in citations:
        score_str = f" ‚Äî ƒë·ªô li√™n quan: {c['score']}" if c.get("score") else ""
        cite_lines.append(f"- **[{c['index']}]** {c['file']} (Trang {c['page']}){score_str}")

    return "\n" + "\n".join(cite_lines)


# ---------------------------------------------------------------------------
# RAG Celery task
# ---------------------------------------------------------------------------

@celery_app.task(name="src.worker.tasks.rag_document_search", bind=True, max_retries=2)
def rag_document_search(
    self,
    question: str,
    session_id: str,
    user_id: str | None = None,
    department: str | None = None,
) -> dict[str, Any]:
    """
    RAG Document Search ‚Äî find and synthesize answers from uploaded documents.
    """
    logger.info(f"[RAG] Search for session {session_id}: {question[:50]}...")
    try:
        from llama_index.core import VectorStoreIndex, StorageContext, Settings
        from llama_index.vector_stores.qdrant import QdrantVectorStore
        from llama_index.embeddings.huggingface import HuggingFaceEmbedding
        from qdrant_client import QdrantClient

        # 1. Setup embedding + vector store
        Settings.embed_model = HuggingFaceEmbedding(model_name="thanhtantran/Vietnamese_Embedding_v2")
        Settings.llm = None
        qdrant = QdrantClient(url=os.getenv("QDRANT_URL", "http://qdrant:6333"))
        vector_store = QdrantVectorStore(client=qdrant, collection_name="port_knowledge")
        storage_ctx = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_vector_store(vector_store=vector_store, storage_context=storage_ctx)

        # 2. Retrieve top-5 chunks
        retriever = index.as_retriever(
            similarity_top_k=5,
            vector_store_kwargs={"filter": _build_qdrant_filter(user_id, department)},
        )
        top_nodes = list(retriever.retrieve(question))[:5]

        # 3. Build context + citations
        citations, context_blocks = _build_context_and_citations(top_nodes)
        context_text = "\n\n---\n\n".join(context_blocks).strip()

        # 4. Synthesize via LLM (with fallback)
        result_text = ""
        if context_text:
            try:
                unified_ctx = _gather_unified_context(question, session_id, user_id)
                result_text = _synthesize_with_llm(
                    question,
                    context_text,
                    long_term_block=unified_ctx.get("long_term_block", ""),
                    summary_block=unified_ctx.get("summary_block", ""),
                    recent_block=unified_ctx.get("recent_block", ""),
                )
            except Exception as e:
                logger.warning(f"[RAG] LLM synthesis failed: {e}")
        if not result_text:
            result_text = _build_fallback_answer(context_blocks)

        result_text += _format_citations_footer(citations)

        # 5. Save via Backend API
        import httpx
        with httpx.Client(timeout=10.0) as http_client:
            resp = http_client.post(
                f"{BACKEND_INTERNAL_URL}/sessions/{session_id}/messages",
                json={"content": result_text, "role": "assistant"},
            )
            resp.raise_for_status()
            logger.info(f"[RAG] Saved answer for session {session_id}")

        from .helpers import publish_task_complete
        publish_task_complete(session_id)
        return {"status": "success", "question": question, "citations": len(citations)}

    except Exception as exc:
        logger.exception(f"Error in RAG document search: {exc}")
        # Return Vietnamese error message ‚Äî NEVER expose tracebacks
        try:
            import httpx
            with httpx.Client(timeout=10.0) as client:
                client.post(
                    f"{BACKEND_INTERNAL_URL}/sessions/{session_id}/messages",
                    json={
                        "content": "Xin l·ªói, h·ªá th·ªëng g·∫∑p s·ª± c·ªë khi t√¨m ki·∫øm t√†i li·ªáu. Vui l√≤ng th·ª≠ l·∫°i sau ·∫°.",
                        "role": "assistant",
                    },
                )
        except Exception:
            pass
        # Still notify frontend so it stops waiting
        from .helpers import publish_task_complete
        publish_task_complete(session_id)
        return {"status": "error", "message": str(exc)}


@celery_app.task(name="src.worker.tasks.process_feedback", bind=True, max_retries=2)
def process_feedback(
    self,
    message_id: str,
    is_liked: bool,
    reason: str | None = None,
) -> dict[str, Any]:
    """
    Self-Correction: Process user feedback on bot answers.
    If disliked ‚Üí find related vectors in Qdrant and mark as low_quality.
    """
    logger.info(f"[chat_priority] Processing feedback for message {message_id}: liked={is_liked}")
    try:
        if is_liked:
            return {"status": "ok", "action": "positive_feedback"}

        # Negative feedback ‚Üí find and downgrade related vectors
        from sqlalchemy import create_engine, text as sql_text
        db_url = os.getenv("DATABASE_URL", "").replace("postgresql+asyncpg://", "postgresql://")
        engine = create_engine(db_url)

        # 1. Get the disliked message content
        with engine.connect() as conn:
            result = conn.execute(
                sql_text("SELECT content, session_id FROM chat_messages WHERE id = :msg_id"),
                {"msg_id": message_id},
            )
            row = result.fetchone()
        if not row:
            return {"status": "error", "message": "Message not found"}

        msg_content = row[0]

        # 2. Embed the message to find matching vectors
        import httpx
        mem0_url = os.getenv("MEM0_URL", MEM0_DEFAULT_URL)
        with httpx.Client(timeout=30.0) as client:
            resp = client.post(
                f"{mem0_url.rstrip('/')}/embed",
                json={"text": msg_content[:500]},
            )
            resp.raise_for_status()
            query_vector = resp.json()["vector"]

        # 3. Search for matching vectors in port_knowledge and chat_chunks
        from src.core.qdrant_setup import get_qdrant_client
        from qdrant_client.http import models as qmodels

        qdrant = get_qdrant_client()
        for collection in ["port_knowledge", "chat_chunks"]:
            try:
                matches = qdrant.query_points(
                    collection_name=collection,
                    query=query_vector,
                    limit=3,
                ).points

                for point in matches:
                    if point.score and point.score > 0.7:
                        qdrant.set_payload(
                            collection_name=collection,
                            payload={"quality": "low", "dislike_reason": reason or "unknown"},
                            points=[point.id],
                        )
                        logger.info(
                            f"[feedback] Marked vector {point.id} in {collection} "
                            f"as low_quality (reason: {reason})"
                        )
            except Exception as e:
                logger.warning(f"[feedback] Error processing {collection}: {e}")

        return {"status": "ok", "action": "vectors_downgraded", "message_id": message_id}

    except Exception as exc:
        logger.exception(f"Error processing feedback: {exc}")
        return {"status": "error", "message": str(exc)}


# =============================================================================
# üî¥ QUEUE: chat_priority ‚Äî Session Summary (Async)
# =============================================================================

@celery_app.task(name="src.worker.tasks.summarize_session_history", bind=True, max_retries=2)
def summarize_session_history(
    self,
    session_id: str,
) -> dict[str, Any]:
    """
    T√≥m t·∫Øt b·∫•t ƒë·ªìng b·ªô l·ªãch s·ª≠ h·ªôi tho·∫°i.
    Triggered every 10 messages ‚Äî runs in background, user doesn't wait.
    
    1. Fetch ALL messages from DB
    2. Call LLM to produce a 500-char summary
    3. Store summary in session.metadata.summary
    """
    logger.info(f"[summary] Summarizing session {session_id}")
    try:
        import httpx
        from sqlalchemy import create_engine, text as sql_text

        db_url = os.getenv("DATABASE_URL", "").replace("postgresql+asyncpg://", "postgresql://")
        engine = create_engine(db_url)

        # 1. Fetch all messages
        with engine.connect() as conn:
            result = conn.execute(
                sql_text(
                    "SELECT role, content FROM chat_messages "
                    "WHERE session_id = :sid ORDER BY created_at ASC"
                ),
                {"sid": session_id},
            )
            rows = result.fetchall()

        if not rows:
            return {"status": "skip", "reason": "no messages"}

        msg_count = len(rows)

        # Truncate each message for the summary prompt (max 200 chars each)
        conversation = "\n".join(
            f"{r[0].upper()}: {r[1][:200]}{'...' if len(r[1]) > 200 else ''}"
            for r in rows
        )

        # 2. Call LLM to summarize
        openai_key = os.getenv("OPENAI_API_KEY", "")
        openai_base = os.getenv("OPENAI_BASE_URL", "https://openrouter.ai/api/v1")
        llm_model = os.getenv("LLM_MODEL", "openai/gpt-5-nano")

        with httpx.Client(timeout=60.0) as client:
            resp = client.post(
                f"{openai_base.rstrip('/')}/chat/completions",
                headers={
                    "Authorization": f"Bearer {openai_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": llm_model,
                    "messages": [
                        {
                            "role": "system",
                            "content": (
                                "B·∫°n l√† chuy√™n gia t√≥m t·∫Øt h·ªôi tho·∫°i. "
                                "T√≥m t·∫Øt cu·ªôc h·ªôi tho·∫°i sau th√†nh M·ªòT ƒëo·∫°n vƒÉn ng·∫Øn (t·ªëi ƒëa 500 k√Ω t·ª±). "
                                "T·∫≠p trung v√†o: ch·ªß ƒë·ªÅ ch√≠nh, th√¥ng tin quan tr·ªçng, v√† k·∫øt lu·∫≠n. "
                                "Vi·∫øt b·∫±ng ti·∫øng Vi·ªát, s√∫c t√≠ch."
                            ),
                        },
                        {"role": "user", "content": conversation[:6000]},  # Cap input
                    ],
                    "temperature": 0.1,
                    "max_tokens": 300,
                },
            )
            resp.raise_for_status()
            summary = resp.json()["choices"][0]["message"]["content"].strip()

        logger.info(f"[summary] Generated summary ({len(summary)} chars) for session {session_id}")

        # 3. Store summary in session metadata
        with engine.connect() as conn:
            # PostgreSQL JSON merge
            conn.execute(
                sql_text(
                    "UPDATE chat_sessions SET metadata = "
                    "COALESCE(metadata, '{}'::json)::jsonb || :patch "
                    "WHERE id = :sid"
                ),
                {
                    "sid": session_id,
                    "patch": f'{{"summary": {__import__("json").dumps(summary)}, "message_count_at_summary": {msg_count}}}',
                },
            )
            conn.commit()

        return {"status": "ok", "session_id": session_id, "summary_length": len(summary)}

    except Exception as exc:
        logger.exception(f"Error summarizing session: {exc}")
        return {"status": "error", "message": str(exc)}
